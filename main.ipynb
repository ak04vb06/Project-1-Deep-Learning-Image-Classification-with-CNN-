{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ab947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ce06cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (1931629403.py, line 15)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfolder_path = os.path.join(dataset_path, folder)\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from translate import translate\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "# 1- Build DataFrame with translated labels\n",
    "\n",
    "dataset_path = r\"/Users/ia_dev/Desktop/Work/archive/raw-img\"\n",
    "data = []\n",
    "\n",
    "# Iterate through each folder in the dataset directory\n",
    "for folder in os.listdir(dataset_path):\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "\n",
    "    # Check if the current path is actually a directory\n",
    "if os.path.isdir(folder_path):\n",
    "        # Get the label from the 'translate' dict (not shown), or default to folder name\n",
    "        label = translate.get(folder, folder)\n",
    "\n",
    "        # Iterate through files inside the specific category folder\n",
    "        for file in os.listdir(folder_path):\n",
    "            # Filter specifically for image formats\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                img_path = os.path.join(folder_path, file)\n",
    "                # Store the full path and its corresponding label\n",
    "                data.append([img_path, label])  \n",
    "# Create a Pandas DataFrame to manage metadata\n",
    "df = pd.DataFrame(data, columns=[\"image_path\", \"label\"])\n",
    "\n",
    "# Display the first few rows and distribution stats\n",
    "print(df.head())\n",
    "print(\"Total images:\", len(df))\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# 2- Train / Test Split (90/10)\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.1,\n",
    "    stratify=df['label'],\n",
    "    random_state=42\n",
    ")\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Test size:\", len(test_df))\n",
    "\n",
    "# 3- Function to convert DataFrame to tf.data.Dataset\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def df_to_dataset(df, shuffle=True, batch_size=32):\n",
    "# Extract file paths and convert labels from strings to integer codes\n",
    "    paths = df['image_path'].values\n",
    "    labels = df['label'].astype('category').cat.codes.values  # string -> integer\n",
    "    # Create a basic TensorSliceDataset\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "\n",
    "    # Internal function to load and preprocess images\n",
    "    def process_path(path, label):\n",
    "        # Read the raw file from disk\n",
    "        img = tf.io.read_file(path)\n",
    "        # Decode the JPEG format into a tensor (0-255 integers)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        # Resize image to the input size required by the model (224x224)\n",
    "        img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "        # Normalize pixel values to range [0, 1] (crucial for convergence)\n",
    "        img = img / 255.0\n",
    "        return img, label\n",
    "\n",
    "    # Apply the processing function in parallel using available CPU cores\n",
    "    ds = ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Shuffle the dataset to prevent the model from learning order-based patterns\n",
    "if shuffle:\n",
    "ds = ds.shuffle(buffer_size=len(df))\n",
    "\n",
    "    # Group examples into batches\n",
    "ds = ds.batch(batch_size)\n",
    "    # Prefetch data to GPU memory while the CPU prepares the next batch (pipeline optimization)\n",
    "ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "return ds\n",
    "\n",
    "# 4- Create TensorFlow Datasets\n",
    "\n",
    "# Create the actual dataset objects ready for training\n",
    "train_dataset = df_to_dataset(train_df, batch_size=BATCH_SIZE)\n",
    "# Do not shuffle test data.\n",
    "test_dataset = df_to_dataset(test_df, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 5- Quick verification\n",
    "\n",
    "# Take 1 batch from the training set to verify shapes and values\n",
    "for images, labels in train_dataset.take(1):\n",
    "    print(\"Image batch shape:\", images.shape)\n",
    "    print(\"Label batch shape:\", labels.shape)\n",
    "    print(\"Example labels:\", labels[:10].numpy())\n",
    "    print(\"Min pixel:\", images.numpy().min())\n",
    "    print(\"Max pixel:\", images.numpy().max())\n",
    "\n",
    "# Visualize the first image\n",
    "img = images[0].numpy()\n",
    "Image.fromarray((img * 255).astype(\"uint8\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
